"""
Citation:
=======================================================================================
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
=======================================================================================
= Author: Olivier Grisel <olivier.grisel@ensta.org>
=         Lars Buitinck
=         Chyi-Kwei Yau <chyikwei.yau@gmail.com>
= License: BSD 3 clause
=======================================================================================
"""

from __future__ import print_function
from time import time
import unicodecsv as csv

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
from nltk.corpus import stopwords


n_samples = 1000000
NMF_topics = 50
NMF_top_words = 15

LDA_topics = 100
LDA_top_words = 10

in_path = '../input'
out_path = '../output'


def print_top_words(model, feature_names, n_top_words, file_name):
    with open('%s/%s'%(out_path,file_name),'w') as source:
        for topic_idx, topic in enumerate(model.components_):
            source.write("Topic #%d: \n" %(topic_idx+1))
            raw_list = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
            weight = [str(topic[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]
            source.write(",".join(raw_list))
            source.write('\n')
            source.write(",".join(weight))
            source.write('\n')
            source.write('Maximum difference: %d'%(float(weight[0])-float(weight[1])))
            source.write('\n')
            source.write('\n')


# Load the document containing descriptions selected at random
def load_dataset(file_name):

    print("Loading dataset...")
    t0 = time()
    data = []
    with open('%s/%s'%(in_path,file_name),'r') as source:
        for each in source:
            data.append(each)

    print("size of data_samples: %s" %(len(data)))

    print("done in %0.3fs." % (time() - t0))
    return data


def obtain_features():
    # import stop words set from NLTK package
    stop_word_set = set(stopwords.words('english'))
    with open('../input/vocab.txt',str('rb')) as source:
        rdr = csv.reader(source, encoding='utf-8')
        features = []
        skill_set = set()
        for row in rdr:
            skill = row[0].lower()
            if skill not in skill_set and skill not in stop_word_set:
                skill_set.add(skill)
                features.append(skill)
    return features


def NMF_cluster(data, min_n, max_n, keyword, vocab):
    # n_features = len(vocab)
    n_features = 5000
    # Use tf-idf features for NMF.
    print("\nExtracting tf-idf features for NMF...")
    tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1,
                                       #vocabulary=vocab,
                                       stop_words='english',
                                       #stop_words=None,
                                       token_pattern=r"(?u)\S+",
                                       max_features=n_features, sublinear_tf=True,
                                       ngram_range=(min_n, max_n))
    t0 = time()
    tfidf = tfidf_vectorizer.fit_transform(data)
    # print out features generated by tf-idf vectorizer
    feature = tfidf_vectorizer.get_feature_names()
    with open('%s/%s' % (out_path, 'features.txt'), 'w+') as source:
        for each in feature:
            source.write(each)
            source.write('\n')

    print('# of features: %d'%(len(tfidf_vectorizer.get_feature_names())))
    print("done in %0.3fs." % (time() - t0))

    # Fit the NMF model
    print("\nFitting the NMF model with tf-idf features"
          "\nn_samples=%d and n_features=%d..."
          % (n_samples, n_features))
    t0 = time()
    nmf = NMF(n_components=NMF_topics, init='nndsvda',
              random_state=1,
              alpha=.1, l1_ratio=.5).fit(tfidf)
    print("done in %0.3fs." % (time() - t0))

    print("\n===NMF model completed===")
    tfidf_feature_names = tfidf_vectorizer.get_feature_names()
    print_top_words(nmf, tfidf_feature_names, NMF_top_words
                    , "%s_Topics in NMF_%d_%d_%s.txt" % (NMF_topics, min_n, max_n, keyword))


if __name__ == "__main__":
    keyword = 'ACCOUNTANT'
    data_samples = load_dataset('%s_DESCRIPTIONS.txt'%(keyword))
    vocab = obtain_features()
    NMF_cluster(data_samples, 1, 3, keyword, vocab)